{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0860aeb5",
   "metadata": {
    "papermill": {
     "duration": 0.00441,
     "end_time": "2025-05-25T07:57:03.227118",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.222708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b231d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.234947Z",
     "iopub.status.busy": "2025-05-25T07:57:03.234747Z",
     "iopub.status.idle": "2025-05-25T07:57:03.238786Z",
     "shell.execute_reply": "2025-05-25T07:57:03.238078Z"
    },
    "papermill": {
     "duration": 0.00912,
     "end_time": "2025-05-25T07:57:03.239856",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.230736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = \"/kaggle/input/icar-data/0325updated.task1train(626p)/\"\n",
    "output_dir = \"/kaggle/working/EAST/data\"  \n",
    "os.makedirs(output_dir, exist_ok=True) \n",
    "\n",
    "image_paths = []\n",
    "boxes = []\n",
    "\n",
    "for filename in sorted(os.listdir(data_dir)):\n",
    "    if filename.endswith('.jpg'):\n",
    "        img_path = os.path.join(data_dir, filename)\n",
    "        txt_path = img_path.replace('.jpg', '.txt')\n",
    "\n",
    "        if not os.path.exists(txt_path):\n",
    "            continue\n",
    "\n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        image_boxes = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                coords = list(map(int, parts[:8])) \n",
    "                image_boxes.append(coords)  \n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "        if image_boxes:\n",
    "            image_paths.append(img_path)\n",
    "            boxes.append(image_boxes)\n",
    "\n",
    "with open(os.path.join(output_dir, 'images.json'), 'w') as f:\n",
    "    json.dump(image_paths, f, indent=2)\n",
    "\n",
    "with open(os.path.join(output_dir, 'boxes.json'), 'w') as f:\n",
    "    json.dump(boxes, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdabc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.247951Z",
     "iopub.status.busy": "2025-05-25T07:57:03.247759Z",
     "iopub.status.idle": "2025-05-25T07:57:03.254062Z",
     "shell.execute_reply": "2025-05-25T07:57:03.253412Z"
    },
    "papermill": {
     "duration": 0.011751,
     "end_time": "2025-05-25T07:57:03.255153",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.243402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from shapely.geometry import Polygon\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def parse_annotation(annotation_path):\n",
    "    boxes = list()\n",
    "\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split(',')\n",
    "\n",
    "            x1 = line[0]\n",
    "            y1 = line[1]\n",
    "            x2 = line[2]\n",
    "            y2 = line[3]\n",
    "            x3 = line[4]\n",
    "            y3 = line[5]\n",
    "            x4 = line[6]\n",
    "            y4 = line[7]\n",
    "\n",
    "            cor = [x1, y1, x2, y2, x3, y3, x4, y4]\n",
    "            boxes.append(cor)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def create_json_data(directory='data/raw_data/train', images_json='EAST/data/images.json',\n",
    "                     boxes_json='EAST/data/boxes.json'):\n",
    "    image_paths = list()\n",
    "    boxes = list()\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filename = filename[: -4]\n",
    "\n",
    "        if filename + '.txt' not in os.listdir(directory):\n",
    "            continue\n",
    "\n",
    "        if filename + '.jpg' not in os.listdir(directory):\n",
    "            continue\n",
    "\n",
    "        image_paths.append(os.path.join(directory, filename + '.jpg'))\n",
    "        boxes.append(parse_annotation(directory + '/' + filename + '.txt'))\n",
    "\n",
    "    with open(images_json, 'w') as f:\n",
    "        json.dump(image_paths, f)\n",
    "\n",
    "    with open(boxes_json, 'w') as f:\n",
    "        json.dump(boxes, f)\n",
    "\n",
    "\n",
    "def cal_distance(x1, y1, x2, y2):\n",
    "\n",
    "    return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "\n",
    "\n",
    "def move_points(vertices, index1, index2, r, coef):\n",
    "    index1 = index1 % 4\n",
    "    index2 = index2 % 4\n",
    "    x1_index = index1 * 2 + 0\n",
    "    y1_index = index1 * 2 + 1\n",
    "    x2_index = index2 * 2 + 0\n",
    "    y2_index = index2 * 2 + 1\n",
    "\n",
    "    r1 = r[index1]\n",
    "    r2 = r[index2]\n",
    "    length_x = vertices[x1_index] - vertices[x2_index]\n",
    "    length_y = vertices[y1_index] - vertices[y2_index]\n",
    "    length = cal_distance(vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index])\n",
    "    if length > 1:\n",
    "        ratio = (r1 * coef) / length\n",
    "        vertices[x1_index] += ratio * (-length_x)\n",
    "        vertices[y1_index] += ratio * (-length_y)\n",
    "        ratio = (r2 * coef) / length\n",
    "        vertices[x2_index] += ratio * length_x\n",
    "        vertices[y2_index] += ratio * length_y\n",
    "    return vertices\n",
    "\n",
    "\n",
    "def shrink_poly(vertices, coef=0.3):\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    r1 = min(cal_distance(x1, y1, x2, y2), cal_distance(x1, y1, x4, y4))\n",
    "    r2 = min(cal_distance(x2, y2, x1, y1), cal_distance(x2, y2, x3, y3))\n",
    "    r3 = min(cal_distance(x3, y3, x2, y2), cal_distance(x3, y3, x4, y4))\n",
    "    r4 = min(cal_distance(x4, y4, x1, y1), cal_distance(x4, y4, x3, y3))\n",
    "    r = [r1, r2, r3, r4]\n",
    "\n",
    "    # obtain offset to perform move_points() automatically\n",
    "    if cal_distance(x1, y1, x2, y2) + cal_distance(x3, y3, x4, y4) > \\\n",
    "            cal_distance(x2, y2, x3, y3) + cal_distance(x1, y1, x4, y4):\n",
    "        offset = 0  # two longer edges are (x1y1-x2y2) & (x3y3-x4y4)\n",
    "    else:\n",
    "        offset = 1  # two longer edges are (x2y2-x3y3) & (x4y4-x1y1)\n",
    "\n",
    "    v = vertices.copy()\n",
    "    v = move_points(v, 0 + offset, 1 + offset, r, coef)\n",
    "    v = move_points(v, 2 + offset, 3 + offset, r, coef)\n",
    "    v = move_points(v, 1 + offset, 2 + offset, r, coef)\n",
    "    v = move_points(v, 3 + offset, 4 + offset, r, coef)\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_rotate_mat(theta):\n",
    "\n",
    "    return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\n",
    "\n",
    "\n",
    "def rotate_vertices(vertices, theta, anchor=None):\n",
    "    v = vertices.reshape((4, 2)).T\n",
    "    if anchor is None:\n",
    "        anchor = v[:, :1]\n",
    "    rotate_map = get_rotate_mat(theta)\n",
    "    res = np.dot(rotate_map, v - anchor)\n",
    "\n",
    "    return (res + anchor).T.reshape(-1)\n",
    "\n",
    "\n",
    "def get_boundary(vertices):\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "\n",
    "    x_min = min(x1, x2, x3, x4)\n",
    "    x_max = max(x1, x2, x3, x4)\n",
    "    y_min = min(y1, y2, y3, y4)\n",
    "    y_max = max(y1, y2, y3, y4)\n",
    "\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def cal_error(vertices):\n",
    "    x_min, x_max, y_min, y_max = get_boundary(vertices)\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4 = vertices\n",
    "    err = cal_distance(x1, y1, x_min, y_min) + cal_distance(x2, y2, x_max, y_min) + \\\n",
    "            cal_distance(x3, y3, x_max, y_max) + cal_distance(x4, y4, x_min, y_max)\n",
    "\n",
    "    return err\n",
    "\n",
    "\n",
    "def find_min_rect_angle(vertices):\n",
    "    angle_interval = 1\n",
    "    angle_list = list(range(-90, 90, angle_interval))\n",
    "    area_list = []\n",
    "    for theta in angle_list:\n",
    "        rotated = rotate_vertices(vertices, theta / 180 * math.pi)\n",
    "        x1, y1, x2, y2, x3, y3, x4, y4 = rotated\n",
    "        temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * \\\n",
    "                    (max(y1, y2, y3, y4) - min(y1, y2, y3, y4))\n",
    "        area_list.append(temp_area)\n",
    "\n",
    "    sorted_area_index = sorted(list(range(len(area_list))), key=lambda k: area_list[k])\n",
    "    min_error = float('inf')\n",
    "    best_index = -1\n",
    "    rank_num = 10\n",
    "    # find the best angle with correct orientation\n",
    "    for index in sorted_area_index[:rank_num]:\n",
    "        rotated = rotate_vertices(vertices, angle_list[index] / 180 * math.pi)\n",
    "        temp_error = cal_error(rotated)\n",
    "        if temp_error < min_error:\n",
    "            min_error = temp_error\n",
    "            best_index = index\n",
    "    return angle_list[best_index] / 180 * math.pi\n",
    "\n",
    "\n",
    "def rotate_all_pixels(rotate_mat, anchor_x, anchor_y, length):\n",
    "    x = np.arange(length)\n",
    "    y = np.arange(length)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    x_lin = x.reshape((1, x.size))\n",
    "    y_lin = y.reshape((1, x.size))\n",
    "    coord_mat = np.concatenate((x_lin, y_lin), 0)\n",
    "    rotated_coord = np.dot(rotate_mat, coord_mat - np.array([[anchor_x], [anchor_y]])) + \\\n",
    "                    np.array([[anchor_x], [anchor_y]])\n",
    "    rotated_x = rotated_coord[0, :].reshape(x.shape)\n",
    "    rotated_y = rotated_coord[1, :].reshape(y.shape)\n",
    "    return rotated_x, rotated_y\n",
    "\n",
    "\n",
    "\n",
    "def rotate_img(img, vertices, angle_range=10):\n",
    "    center_x = (img.width - 1) / 2\n",
    "    center_y = (img.height - 1) / 2\n",
    "    angle = angle_range * (np.random.rand() * 2 - 1)\n",
    "    img = img.rotate(angle, Image.BILINEAR)\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        new_vertices[i, :] = rotate_vertices(vertice, -angle / 180 * math.pi, np.array([[center_x], [center_y]]))\n",
    "    return img, new_vertices\n",
    "\n",
    "\n",
    "def resize(img, vertices, length):\n",
    "    shape = img.size\n",
    "    new_image = img.resize((length, length))\n",
    "\n",
    "    new_vertices = np.zeros(vertices.shape)\n",
    "    for i, vertice in enumerate(vertices):\n",
    "      new_vertices[i, [0, 2, 4, 6]] = vertices[i, [0, 2, 4, 6]] * (length / shape[0])\n",
    "      new_vertices[i, [1, 3, 5, 7]] = vertices[i, [1, 3, 5, 7]] * (length / shape[1])\n",
    "    \n",
    "    return new_image, new_vertices\n",
    "\n",
    "\n",
    "def get_score_geo(img, vertices, scale, length):\n",
    "    score_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\n",
    "    geo_map = np.zeros((int(img.height * scale), int(img.width * scale), 5), np.float32)\n",
    "\n",
    "    index = np.arange(0, length, int(1 / scale))\n",
    "    index_x, index_y = np.meshgrid(index, index)\n",
    "    ignored_polys = []\n",
    "    polys = []\n",
    "\n",
    "    for i, vertice in enumerate(vertices):\n",
    "        poly = np.around(scale * shrink_poly(vertice).reshape((4, 2))).astype(np.int32)  # scaled & shrinked\n",
    "        polys.append(poly)\n",
    "        temp_mask = np.zeros(score_map.shape[:-1], np.float32)\n",
    "        cv2.fillPoly(temp_mask, [poly], 1)\n",
    "\n",
    "        theta = 0\n",
    "        rotate_mat = get_rotate_mat(theta)\n",
    "\n",
    "        rotated_vertices = rotate_vertices(vertice, theta)\n",
    "        x_min, x_max, y_min, y_max = get_boundary(rotated_vertices)\n",
    "        rotated_x, rotated_y = rotate_all_pixels(rotate_mat, vertice[0], vertice[1], length)\n",
    "\n",
    "        d1 = rotated_y - y_min\n",
    "        d1[d1 < 0] = 0\n",
    "        d2 = y_max - rotated_y\n",
    "        d2[d2 < 0] = 0\n",
    "        d3 = rotated_x - x_min\n",
    "        d3[d3 < 0] = 0\n",
    "        d4 = x_max - rotated_x\n",
    "        d4[d4 < 0] = 0\n",
    "        geo_map[:, :, 0] += d1[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 1] += d2[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 2] += d3[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 3] += d4[index_y, index_x] * temp_mask\n",
    "        geo_map[:, :, 4] += theta * temp_mask\n",
    "\n",
    "    cv2.fillPoly(score_map, polys, 1)\n",
    "\n",
    "    score_map = torch.Tensor(score_map).permute(2, 0, 1)\n",
    "    geo_map = torch.Tensor(geo_map).permute(2, 0, 1)\n",
    "\n",
    "    return score_map, geo_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6da34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.262564Z",
     "iopub.status.busy": "2025-05-25T07:57:03.262374Z",
     "iopub.status.idle": "2025-05-25T07:57:03.265855Z",
     "shell.execute_reply": "2025-05-25T07:57:03.265222Z"
    },
    "papermill": {
     "duration": 0.00835,
     "end_time": "2025-05-25T07:57:03.266866",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.258516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ReceiptDataset(Dataset):\n",
    "    def __init__(self, image_paths, boxes, scale=0.25, length=512):\n",
    "        super(ReceiptDataset, self).__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.boxes = boxes\n",
    "        self.scale = scale\n",
    "        self.length = length\n",
    "        self.tranforms = transforms.Compose([\n",
    "            transforms.ColorJitter(0.5, 0.5, 0.5, 0.25),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                 std=(0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        vertices = np.array(self.boxes[item], dtype=int)\n",
    "\n",
    "        image = Image.open(self.image_paths[item])\n",
    "        image = image.convert('RGB')\n",
    "        # image, vertices = rotate_img(image, vertices)\n",
    "        image, vertices = resize(image, vertices, self.length)\n",
    "\n",
    "        score_map, geo_map = get_score_geo(image, vertices, self.scale, self.length)\n",
    "        image = self.tranforms(image)\n",
    "\n",
    "        return image, score_map, geo_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('EAST/data/images.json', 'r') as f:\n",
    "        image_paths = json.load(f)\n",
    "    with open('EAST/data/boxes.json', 'r') as f:\n",
    "        boxes = json.load(f)\n",
    "\n",
    "    dataset = ReceiptDataset(image_paths=[image_paths[0]], boxes=[boxes[0]])\n",
    "    # print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c510bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.274324Z",
     "iopub.status.busy": "2025-05-25T07:57:03.274138Z",
     "iopub.status.idle": "2025-05-25T07:57:03.277525Z",
     "shell.execute_reply": "2025-05-25T07:57:03.276902Z"
    },
    "papermill": {
     "duration": 0.008259,
     "end_time": "2025-05-25T07:57:03.278534",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.270275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def get_dice_loss(gt_score, pred_score):\n",
    "    inter = torch.sum(gt_score * pred_score)\n",
    "    union = torch.sum(gt_score) + torch.sum(pred_score) + 1e-5\n",
    "\n",
    "    return 1. - 2 * inter / union\n",
    "\n",
    "\n",
    "def get_geo_loss(gt_geo, pred_geo):\n",
    "    d1_gt, d2_gt, d3_gt, d4_gt, angle_gt = torch.split(gt_geo, 1, 1)\n",
    "    d1_pred, d2_pred, d3_pred, d4_pred, angle_pred = torch.split(pred_geo, 1, 1)\n",
    "    area_gt = (d1_gt + d2_gt) * (d3_gt + d4_gt)\n",
    "    area_pred = (d1_pred + d2_pred) * (d3_pred + d4_pred)\n",
    "    w_inter = torch.min(d1_gt, d1_pred) + torch.min(d2_gt, d2_pred)\n",
    "    h_inter = torch.min(d3_gt, d3_pred) + torch.min(d4_gt, d4_pred)\n",
    "    area_inter = w_inter * h_inter\n",
    "    area_union = area_gt + area_pred - area_inter\n",
    "    iou_loss_map = - torch.log((area_inter + 1.0) / (area_union + 1.0))\n",
    "    angle_loss_map = 1 - torch.cos(angle_gt - angle_pred)\n",
    "\n",
    "    return iou_loss_map, angle_loss_map\n",
    "\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, weight_angle=10):\n",
    "        super(Loss, self).__init__()\n",
    "        self.weight_angle = weight_angle\n",
    "\n",
    "    def forward(self, gt_score, pred_score, gt_geo, pred_geo):\n",
    "        if torch.sum(gt_score) < 1:\n",
    "            return torch.sum(pred_score + pred_geo) * 0\n",
    "\n",
    "        classify_loss = get_dice_loss(gt_score, pred_score)\n",
    "        iou_loss_map, angle_loss_map = get_geo_loss(gt_geo, pred_geo)\n",
    "\n",
    "        angle_loss = torch.sum(angle_loss_map * gt_score) / torch.sum(gt_score)\n",
    "        iou_loss = torch.sum(iou_loss_map * gt_score) / torch.sum(gt_score)\n",
    "        geo_loss = self.weight_angle * angle_loss + iou_loss\n",
    "\n",
    "        return geo_loss + classify_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962eeac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.286323Z",
     "iopub.status.busy": "2025-05-25T07:57:03.286088Z",
     "iopub.status.idle": "2025-05-25T07:57:03.291736Z",
     "shell.execute_reply": "2025-05-25T07:57:03.291046Z"
    },
    "papermill": {
     "duration": 0.011001,
     "end_time": "2025-05-25T07:57:03.292934",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.281933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = list()\n",
    "    in_channel = 3\n",
    "\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels=in_channel, out_channels=v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "\n",
    "            in_channel = v\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight, 1)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                nn.init.normal_(layer.weight, 0, 0.01)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(Extractor, self).__init__()\n",
    "        vgg16_bn = VGG(make_layers(cfg, batch_norm=True))\n",
    "\n",
    "        if pretrained:\n",
    "            vgg16_bn.load_state_dict(torch.load('./EAST/data/vgg16_bn.pth'))\n",
    "            print('Model loaded')\n",
    "\n",
    "        self.features = vgg16_bn.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = list()\n",
    "        for layer in self.features:\n",
    "            x = layer(x)\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                out.append(x)\n",
    "\n",
    "        return out[1:]\n",
    "\n",
    "\n",
    "class Merge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Merge, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1024, out_channels=128, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=384, out_channels=64, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=192, out_channels=32, kernel_size=1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.conv6 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(32)\n",
    "        self.relu6 = nn.ReLU()\n",
    "\n",
    "        self.conv7 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(32)\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.interpolate(x[3], scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[2]), dim=1)\n",
    "        y = self.relu1(self.bn1(self.conv1(y)))\n",
    "        y = self.relu2(self.bn2(self.conv2(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[1]), dim=1)\n",
    "        y = self.relu3(self.bn3(self.conv3(y)))\n",
    "        y = self.relu4(self.bn4(self.conv4(y)))\n",
    "\n",
    "        y = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        y = torch.cat((y, x[0]), dim=1)\n",
    "        y = self.relu5(self.bn5(self.conv5(y)))\n",
    "        y = self.relu6(self.bn6(self.conv6(y)))\n",
    "\n",
    "        y = self.relu7(self.bn7(self.conv7(y)))\n",
    "\n",
    "        return y\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight, 1)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "class Output(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=4, kernel_size=1)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "\n",
    "        self.scope = 512\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        score = self.sigmoid1(self.conv1(x)) \n",
    "        loc = self.sigmoid2(self.conv2(x)) * self.scope\n",
    "        angle = (self.sigmoid3(self.conv3(x)) - 0.5) * math.pi\n",
    "\n",
    "        geo = torch.cat((loc, angle), dim=1)\n",
    "\n",
    "        return score, geo\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "class East(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super(East, self).__init__()\n",
    "        self.extractor = Extractor(pretrained)\n",
    "        self.merge = Merge()\n",
    "        self.output = Output()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extractor(x)\n",
    "        x = self.merge(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('EAST/data/images.json', 'r') as f:\n",
    "        image_paths = json.load(f)\n",
    "    with open('EAST/data/boxes.json', 'r') as f:\n",
    "        boxes = json.load(f)\n",
    "\n",
    "    dataset = ReceiptDataset(image_paths[: 2], boxes[: 2])\n",
    "    model = East()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad8a1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.300709Z",
     "iopub.status.busy": "2025-05-25T07:57:03.300521Z",
     "iopub.status.idle": "2025-05-25T07:57:03.304444Z",
     "shell.execute_reply": "2025-05-25T07:57:03.303926Z"
    },
    "papermill": {
     "duration": 0.009131,
     "end_time": "2025-05-25T07:57:03.305529",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.296398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Split train test\n",
    "IMAGE_PATHS = 'EAST/data/images.json'\n",
    "BOXES = 'EAST/data/boxes.json'\n",
    "\n",
    "with open(IMAGE_PATHS, 'r') as f:\n",
    "    image_paths = json.load(f)\n",
    "with open(BOXES, 'r') as f:\n",
    "    boxes = json.load(f)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(image_paths, boxes,\n",
    "                                                  test_size=0.35, shuffle=True, random_state=2021)\n",
    "train_dataset = ReceiptDataset(X_train, y_train)\n",
    "val_dataset = ReceiptDataset(X_val, y_val)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Model\n",
    "EPOCHS = 80\n",
    "\n",
    "model = East()\n",
    "model = model.to(device)\n",
    "model = torch.nn.DataParallel(model)\n",
    "# model.load_state_dict(torch.load('/kaggle/working/east_best.pt.pt'))\n",
    "lr = 1e-4\n",
    "loss_fn = Loss().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch {}'.format(epoch + 1))\n",
    "\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    for batch_idx, (X_batch_train, gt_score, gt_geo) in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "        X_batch_train = X_batch_train.to(device)\n",
    "        gt_score = gt_score.to(device)\n",
    "        gt_geo = gt_geo.to(device)\n",
    "        pred_score, pred_geo = model(X_batch_train)\n",
    "\n",
    "        loss = loss_fn(gt_score, pred_score, gt_geo, pred_geo)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "\n",
    "    avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
    "    train_loss.append(avg_train_loss)\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, gt_score, gt_geo in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "            X_batch = X_batch.to(device)\n",
    "            gt_score = gt_score.to(device)\n",
    "            gt_geo = gt_geo.to(device)\n",
    "\n",
    "            pred_score, pred_geo = model(X_batch)\n",
    "            loss = loss_fn(gt_score, pred_score, gt_geo, pred_geo)\n",
    "            epoch_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = epoch_val_loss / len(val_dataloader)\n",
    "    val_loss.append(avg_val_loss)\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'east_best.pt')\n",
    "        print(f\"Saved best model (Val Loss: {best_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d018658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.313574Z",
     "iopub.status.busy": "2025-05-25T07:57:03.312977Z",
     "iopub.status.idle": "2025-05-25T07:57:03.316142Z",
     "shell.execute_reply": "2025-05-25T07:57:03.315448Z"
    },
    "papermill": {
     "duration": 0.008147,
     "end_time": "2025-05-25T07:57:03.317291",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.309144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install lanms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acccd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.325246Z",
     "iopub.status.busy": "2025-05-25T07:57:03.325042Z",
     "iopub.status.idle": "2025-05-25T07:57:03.329579Z",
     "shell.execute_reply": "2025-05-25T07:57:03.329058Z"
    },
    "papermill": {
     "duration": 0.009721,
     "end_time": "2025-05-25T07:57:03.330579",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.320858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import lanms\n",
    "\n",
    "\n",
    "\n",
    "def resize_img(img):\n",
    "\tw, h = img.size\n",
    "\tresize_w = w\n",
    "\tresize_h = h\n",
    "\n",
    "\tresize_h = resize_h if resize_h % 32 == 0 else int(resize_h / 32) * 32\n",
    "\tresize_w = resize_w if resize_w % 32 == 0 else int(resize_w / 32) * 32\n",
    "\timg = img.resize((resize_w, resize_h), Image.BILINEAR)\n",
    "\tratio_h = resize_h / h\n",
    "\tratio_w = resize_w / w\n",
    "\n",
    "\treturn img, ratio_h, ratio_w\n",
    "\n",
    "\n",
    "def load_pil(img):\n",
    "\tt = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\treturn t(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "def is_valid_poly(res, score_shape, scale):\n",
    "\tcnt = 0\n",
    "\tfor i in range(res.shape[1]):\n",
    "\t\tif res[0, i] < 0 or res[0, i] >= score_shape[1] * scale or \\\n",
    "\t\t\t\tres[1, i] < 0 or res[1, i] >= score_shape[0] * scale:\n",
    "\t\t\tcnt += 1\n",
    "\treturn True if cnt <= 1 else False\n",
    "\n",
    "\n",
    "def restore_polys(valid_pos, valid_geo, score_shape, scale=4):\n",
    "\tpolys = []\n",
    "\tindex = []\n",
    "\tvalid_pos *= scale\n",
    "\td = valid_geo[:4, :]  # 4 x N\n",
    "\n",
    "\tfor i in range(valid_pos.shape[0]):\n",
    "\t\tx = valid_pos[i, 0]\n",
    "\t\ty = valid_pos[i, 1]\n",
    "\t\ty_min = y - d[0, i] * 1.3\n",
    "\t\ty_max = y + d[1, i] * 1.3\n",
    "\t\tx_min = x - d[2, i] * 1.1\n",
    "\t\tx_max = x + d[3, i] * 1.1\n",
    "\n",
    "\t\ttemp_x = np.array([[x_min, x_max, x_max, x_min]])\n",
    "\t\ttemp_y = np.array([[y_min, y_min, y_max, y_max]])\n",
    "\n",
    "\t\tcoordinate = np.concatenate((temp_x, temp_y), axis=0)\n",
    "\n",
    "\t\tif is_valid_poly(coordinate, score_shape, scale):\n",
    "\t\t\tindex.append(i)\n",
    "\t\t\tpolys.append([coordinate[0, 0], coordinate[1, 0], coordinate[0, 1], coordinate[1, 1],\n",
    "\t\t\t\t\t\tcoordinate[0, 2], coordinate[1, 2], coordinate[0, 3], coordinate[1, 3]])\n",
    "\n",
    "\treturn np.array(polys), index\n",
    "\n",
    "\n",
    "def get_boxes(score, geo, score_thresh=0.9, nms_thresh=0.2):\n",
    "\tscore = score[0, :, :]\n",
    "\txy_text = np.argwhere(score > score_thresh)\n",
    "\tif xy_text.size == 0:\n",
    "\t\treturn None\n",
    "\n",
    "\txy_text = xy_text[np.argsort(xy_text[:, 0])]\n",
    "\tvalid_pos = xy_text[:, ::-1].copy()\n",
    "\tvalid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]]\n",
    "\tpolys_restored, index = restore_polys(valid_pos, valid_geo, score.shape) \n",
    "\tif polys_restored.size == 0:\n",
    "\t\treturn None\n",
    "\n",
    "\tboxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32)\n",
    "\tboxes[:, :8] = polys_restored\n",
    "\tboxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]]\n",
    "\tboxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thresh)\n",
    "\n",
    "\treturn boxes\n",
    "\n",
    "\n",
    "def adjust_ratio(boxes, ratio_w, ratio_h):\n",
    "\tif boxes is None or boxes.size == 0:\n",
    "\t\treturn None\n",
    "\tboxes[:, [0, 2, 4, 6]] /= ratio_w\n",
    "\tboxes[:, [1, 3, 5, 7]] /= ratio_h\n",
    "\treturn np.around(boxes)\n",
    "\t\n",
    "\t\n",
    "def detect(img, model, device):\n",
    "\timg, ratio_h, ratio_w = resize_img(img)\n",
    "\twith torch.no_grad():\n",
    "\t\tscore, geo = model(load_pil(img).to(device))\n",
    "\tboxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy())\n",
    "\treturn adjust_ratio(boxes, ratio_w, ratio_h)\n",
    "\n",
    "\n",
    "def plot_boxes(img, boxes):\n",
    "\tif boxes is None:\n",
    "\t\treturn img\n",
    "\t\n",
    "\tdraw = ImageDraw.Draw(img)\n",
    "\tfor box in boxes:\n",
    "\t\tdraw.polygon([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]], outline=(0, 255, 0))\n",
    "\treturn img\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = '/kaggle/working/east_best.pt'\n",
    "    img_path = '/kaggle/input/icar-data/0325updated.task1train(626p)/X00016469612.jpg'\n",
    "    res_img = './res.png'\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = East().to(device)\n",
    "    state_dict = torch.load('/kaggle/working/east_best.pt', map_location=device)\n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            k = k[7:]  \n",
    "        new_state_dict[k] = v\n",
    "    \n",
    "    # Load vào model\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    boxes = detect(img, model, device)\n",
    "\n",
    "    print(boxes)\n",
    "\n",
    "    plot_img = plot_boxes(img, boxes)\n",
    "    plot_img.save(res_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a583809",
   "metadata": {
    "papermill": {
     "duration": 0.003233,
     "end_time": "2025-05-25T07:57:03.337259",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.334026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767cdf93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.345070Z",
     "iopub.status.busy": "2025-05-25T07:57:03.344886Z",
     "iopub.status.idle": "2025-05-25T07:57:03.358392Z",
     "shell.execute_reply": "2025-05-25T07:57:03.357753Z"
    },
    "papermill": {
     "duration": 0.018729,
     "end_time": "2025-05-25T07:57:03.359461",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.340732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def parse_annotation(annotation):\n",
    "    boxes = list()\n",
    "    texts = list()\n",
    "\n",
    "    with open(annotation, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line_spl = line.split(',')\n",
    "\n",
    "            full_box = [int(line_spl[i]) for i in range(8)]\n",
    "            box_len = ' '.join([str(element) for element in full_box])\n",
    "            text = line[len(box_len) + 1:-1]\n",
    "\n",
    "            box = [full_box[0], full_box[1], full_box[4], full_box[5]]\n",
    "\n",
    "            boxes.append(box)\n",
    "            texts.append(text)\n",
    "\n",
    "    return boxes, texts\n",
    "\n",
    "\n",
    "def create_data(data_directory='data/raw_data/train',\n",
    "                image_directory='data/task2/image', annotation_directory='data/task2/annotation'):\n",
    "    all_image_paths = list()\n",
    "    all_texts = list()\n",
    "\n",
    "    index = 0\n",
    "    for file in os.listdir(data_directory):\n",
    "        if file.endswith('.txt'):\n",
    "            if file[: -4] + '.jpg' not in os.listdir(data_directory):\n",
    "                continue\n",
    "\n",
    "            annotation = os.path.join(os.getcwd(), data_directory, file)\n",
    "            image_path = os.path.join(os.getcwd(), data_directory, file[: -4] + '.jpg')\n",
    "\n",
    "            boxes, texts = parse_annotation(annotation)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            for i, box in enumerate(boxes):\n",
    "                index += 1\n",
    "                crop_image_path = os.path.join(image_directory, str(index) + '.jpg')\n",
    "                annotation_path = os.path.join(annotation_directory, str(index) + '.txt')\n",
    "                all_image_paths.append(crop_image_path)\n",
    "\n",
    "                crop_image = image.crop(box)\n",
    "                crop_image.save(crop_image_path)\n",
    "\n",
    "                with open(annotation_path, 'w') as f:\n",
    "                    f.write(texts[i])\n",
    "                    all_texts.append(texts[i])\n",
    "\n",
    "    with open(os.path.join(image_directory, 'images.json'), 'w') as f:\n",
    "        json.dump(all_image_paths, f)\n",
    "\n",
    "    with open(os.path.join(annotation_directory, 'texts.json'), 'w') as f:\n",
    "        json.dump(all_texts, f)\n",
    "\n",
    "\n",
    "def create_vocab(annotation_directory='data/task2/annotation'):\n",
    "    vocab = set()\n",
    "\n",
    "    for file in os.listdir(annotation_directory):\n",
    "        if not file.endswith('.txt'):\n",
    "            continue\n",
    "        with open(os.path.join(os.getcwd(), annotation_directory, file), 'r') as f:\n",
    "            text = f.read()\n",
    "            vocab.update(list(text))\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    with open(os.path.join(os.getcwd(), annotation_directory, 'vocab.json'), 'w') as f:\n",
    "        json.dump(list(vocab), f)\n",
    "\n",
    "    # return vocab\n",
    "\n",
    "\n",
    "def create_map(vocab):\n",
    "    map = {i + 1: char for i, char in enumerate(vocab)}\n",
    "    rev_map = {char: i for i, char in map.items()}\n",
    "\n",
    "    return map, rev_map\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    with open('data/task2/annotation/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    map, rev_map = create_map(vocab)\n",
    "\n",
    "    text_encode = [rev_map[text[i]] for i in range(len(text))]\n",
    "\n",
    "    return text_encode, len(text)\n",
    "\n",
    "\n",
    "def decode(labels):\n",
    "    with open('data/task2/annotation/vocab.json', 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "\n",
    "    map, rev_map = create_map(vocab)\n",
    "\n",
    "    text_decode = [map[i] for i in labels]\n",
    "    text_decode = ''.join(text_decode)\n",
    "\n",
    "    return text_decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ee5909",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.366709Z",
     "iopub.status.busy": "2025-05-25T07:57:03.366520Z",
     "iopub.status.idle": "2025-05-25T07:57:03.369967Z",
     "shell.execute_reply": "2025-05-25T07:57:03.369488Z"
    },
    "papermill": {
     "duration": 0.008358,
     "end_time": "2025-05-25T07:57:03.371150",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.362792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/kaggle/working/data/task2/image\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a29174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.378841Z",
     "iopub.status.busy": "2025-05-25T07:57:03.378641Z",
     "iopub.status.idle": "2025-05-25T07:57:03.381758Z",
     "shell.execute_reply": "2025-05-25T07:57:03.381142Z"
    },
    "papermill": {
     "duration": 0.008054,
     "end_time": "2025-05-25T07:57:03.382821",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.374767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/kaggle/working/data/task2/annotation\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f39702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:03.390413Z",
     "iopub.status.busy": "2025-05-25T07:57:03.390214Z",
     "iopub.status.idle": "2025-05-25T07:57:41.663019Z",
     "shell.execute_reply": "2025-05-25T07:57:41.662485Z"
    },
    "papermill": {
     "duration": 38.278,
     "end_time": "2025-05-25T07:57:41.664349",
     "exception": false,
     "start_time": "2025-05-25T07:57:03.386349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_data(data_directory=\"/kaggle/input/icar-data/0325updated.task1train(626p)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e5053e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:41.672546Z",
     "iopub.status.busy": "2025-05-25T07:57:41.672332Z",
     "iopub.status.idle": "2025-05-25T07:57:42.696543Z",
     "shell.execute_reply": "2025-05-25T07:57:42.695768Z"
    },
    "papermill": {
     "duration": 1.029665,
     "end_time": "2025-05-25T07:57:42.697903",
     "exception": false,
     "start_time": "2025-05-25T07:57:41.668238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_vocab(annotation_directory='/kaggle/working/data/task2/annotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4f4949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:42.705881Z",
     "iopub.status.busy": "2025-05-25T07:57:42.705671Z",
     "iopub.status.idle": "2025-05-25T07:57:51.337312Z",
     "shell.execute_reply": "2025-05-25T07:57:51.336490Z"
    },
    "papermill": {
     "duration": 8.636933,
     "end_time": "2025-05-25T07:57:51.338515",
     "exception": false,
     "start_time": "2025-05-25T07:57:42.701582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 280])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ReceiptDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, width=280, height=64):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.transform = ResizeNormalize(width, height)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_path = self.image_paths[item]\n",
    "        image = Image.open(image_path)\n",
    "        image = ImageOps.grayscale(image)\n",
    "        text = self.texts[item]\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "\n",
    "class ResizeNormalize(object):\n",
    "    def __init__(self, width=280, height=64):\n",
    "        self.scale_width = width\n",
    "        self.scale_height = height\n",
    "        self.transforms = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Normalize(mean=0.5,\n",
    "                      std=0.5)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        new_height = self.scale_height\n",
    "        new_width = w * (new_height / h)\n",
    "        new_width = int(new_width)\n",
    "\n",
    "        if new_width >= self.scale_width:\n",
    "            image = image.resize((self.scale_width, self.scale_height))\n",
    "        else:\n",
    "            image = image.resize((new_width, new_height))\n",
    "            image_pad = np.zeros((self.scale_height, self.scale_width))\n",
    "            image_pad[: new_height, : new_width] = image\n",
    "            image = image_pad\n",
    "            image = Image.fromarray(np.uint8(image))\n",
    "\n",
    "        image = self.transforms(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = list()\n",
    "    text_encodes = list()\n",
    "    text_lens = list()\n",
    "    for b in batch:\n",
    "        images.append(b[0])\n",
    "        text_encode, text_len = encode(b[1])\n",
    "        text_encodes += text_encode\n",
    "        text_lens.append(text_len)\n",
    "\n",
    "    return torch.stack(images, dim=0), torch.tensor(text_encodes), torch.tensor(text_lens)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = ReceiptDataset(['data/task2/image/37553.jpg'], ['abc'])\n",
    "    print(dataset[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eac66b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:51.347436Z",
     "iopub.status.busy": "2025-05-25T07:57:51.346926Z",
     "iopub.status.idle": "2025-05-25T07:57:51.369146Z",
     "shell.execute_reply": "2025-05-25T07:57:51.368461Z"
    },
    "papermill": {
     "duration": 0.027708,
     "end_time": "2025-05-25T07:57:51.370273",
     "exception": false,
     "start_time": "2025-05-25T07:57:51.342565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, pretrained=False):\n",
    "        super(ConvolutionLayer, self).__init__()\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=3, padding=1)  # (64, 64, 280)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)  # (64, 64, 280)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # (64, 32, 140)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)  # (128, 32, 140)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)  # (128, 32, 140)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # (128, 16, 70)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)  # (256, 16, 70)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)  # (256, 16, 70)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)  # (256, 16, 70)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # (256, 8, 70)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)  # (512, 8, 70)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.bn4_1 = nn.BatchNorm2d(num_features=512)  # (512, 8, 70)\n",
    "        self.conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)  # (512, 8, 70)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.bn4_2 = nn.BatchNorm2d(num_features=512)  # (512, 8, 70)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))  # (512, 4, 70)\n",
    "        self.conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)  # (512, 4, 70)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1_1(self.conv1_1(x))\n",
    "        out = self.relu1_2(self.conv1_2(out))\n",
    "        out = self.pool1(out)\n",
    "\n",
    "        out = self.relu2_1(self.conv2_1(out))\n",
    "        out = self.relu2_2(self.conv2_2(out))\n",
    "        out = self.pool2(out)\n",
    "\n",
    "        out = self.relu3_1(self.conv3_1(out))\n",
    "        out = self.relu3_2(self.conv3_2(out))\n",
    "        out = self.relu3_3(self.conv3_3(out))\n",
    "        out = self.pool3(out)\n",
    "\n",
    "        out = self.relu4_1(self.conv4_1(out))\n",
    "        out = self.bn4_1(out)\n",
    "        out = self.relu4_2(self.conv4_2(out))\n",
    "        out = self.bn4_2(out)\n",
    "        out = self.relu4_3(self.conv4_3(out))\n",
    "        out = self.pool4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_weight(self):\n",
    "        state_dict = self.state_dict()\n",
    "\n",
    "        pretrained_state_dict = torchvision.models.vgg16(pretrained=self.pretrained).state_dict()\n",
    "\n",
    "        state_dict['conv1_2.weight'] = pretrained_state_dict['features.2.weight']\n",
    "        state_dict['conv1_2.bias'] = pretrained_state_dict['features.2.bias']\n",
    "\n",
    "        state_dict['conv2_1.weight'] = pretrained_state_dict['features.5.weight']\n",
    "        state_dict['conv2_1.bias'] = pretrained_state_dict['features.5.bias']\n",
    "\n",
    "        state_dict['conv2_2.weight'] = pretrained_state_dict['features.7.weight']\n",
    "        state_dict['conv2_2.bias'] = pretrained_state_dict['features.7.bias']\n",
    "\n",
    "        state_dict['conv3_1.weight'] = pretrained_state_dict['features.10.weight']\n",
    "        state_dict['conv3_1.bias'] = pretrained_state_dict['features.10.bias']\n",
    "\n",
    "        state_dict['conv3_2.weight'] = pretrained_state_dict['features.12.weight']\n",
    "        state_dict['conv3_2.bias'] = pretrained_state_dict['features.12.bias']\n",
    "\n",
    "        state_dict['conv3_3.weight'] = pretrained_state_dict['features.14.weight']\n",
    "        state_dict['conv3_3.bias'] = pretrained_state_dict['features.14.bias']\n",
    "\n",
    "        state_dict['conv4_1.weight'] = pretrained_state_dict['features.17.weight']\n",
    "        state_dict['conv4_1.bias'] = pretrained_state_dict['features.17.bias']\n",
    "        state_dict['conv4_2.weight'] = pretrained_state_dict['features.19.weight']\n",
    "        state_dict['conv4_2.bias'] = pretrained_state_dict['features.19.bias']\n",
    "        state_dict['conv4_3.weight'] = pretrained_state_dict['features.21.weight']\n",
    "        state_dict['conv4_3.bias'] = pretrained_state_dict['features.21.bias']\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                nn.init.constant_(layer.weight, 1)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, n_classes, hidden_dim=256):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm1 = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_dim*2, hidden_size=self.hidden_dim,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, self.n_classes)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, pretrained=False, hidden_size=256, n_classes=73):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.convolution_layer = ConvolutionLayer()\n",
    "        self.rnn_layer = RNNLayer(n_classes=n_classes)\n",
    "        self.linear = nn.Linear(in_features=2048, out_features=hidden_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution_layer(x)  # (N, 512, 4, 70)\n",
    "        out = out.permute(0, 3, 1, 2)  # (N, 70, 512, 4)\n",
    "        out = out.view(-1, 70, 2048)  # (N, 70, 2048)\n",
    "        out = self.linear(out)  # (N, 70, 256)\n",
    "        out = out.permute(1, 0, 2)  # (70, N, 256)\n",
    "        out = self.rnn_layer(out)  # (70, N, 73)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model=256, nhead=4, num_layers=2, dim_feedforward=512, dropout=0.1, n_classes=73):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, T, d_model)\n",
    "        x = self.transformer_encoder(x)  # (N, T, d_model)\n",
    "        x = self.fc(x)  # (N, T, n_classes)\n",
    "        return x.permute(1, 0, 2)  # (T, N, n_classes)\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "class CTRN(nn.Module):\n",
    "    def __init__(self, pretrained=False, hidden_size=256, n_classes=73):\n",
    "        super(CTRN, self).__init__()\n",
    "        self.convolution_layer = ConvolutionLayer(pretrained=pretrained)\n",
    "        self.linear = nn.Linear(in_features=2048, out_features=hidden_size)\n",
    "        self.transformer_layer = TransformerLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=4,\n",
    "            num_layers=2,\n",
    "            dim_feedforward=hidden_size * 2,\n",
    "            n_classes=n_classes\n",
    "        )\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution_layer(x)  # (N, 512, 4, 70)\n",
    "        out = out.permute(0, 3, 1, 2)  # (N, 70, 512, 4)\n",
    "        out = out.contiguous().view(out.size(0), out.size(1), -1)  # (N, 70, 2048)\n",
    "        out = self.linear(out)  # (N, 70, 256)\n",
    "        out = self.transformer_layer(out)  # (70, N, n_classes)\n",
    "        return out\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d73d248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:57:51.378938Z",
     "iopub.status.busy": "2025-05-25T07:57:51.378717Z",
     "iopub.status.idle": "2025-05-25T09:39:58.029365Z",
     "shell.execute_reply": "2025-05-25T09:39:58.028681Z"
    },
    "papermill": {
     "duration": 6126.663531,
     "end_time": "2025-05-25T09:39:58.037995",
     "exception": false,
     "start_time": "2025-05-25T07:57:51.374464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 1/30 [03:15<1:34:27, 195.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 1 Loss: 4.3755\n",
      "Saved better model at epoch 1 with loss 4.3755\n",
      "\n",
      "Epoch 2: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 2/30 [06:40<1:33:56, 201.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 2 Loss: 3.4156\n",
      "Saved better model at epoch 2 with loss 3.4156\n",
      "\n",
      "Epoch 3: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 3/30 [10:07<1:31:45, 203.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 3 Loss: 1.9040\n",
      "Saved better model at epoch 3 with loss 1.9040\n",
      "\n",
      "Epoch 4: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 4/30 [13:34<1:28:49, 204.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 4 Loss: 0.4803\n",
      "Saved better model at epoch 4 with loss 0.4803\n",
      "\n",
      "Epoch 5: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 5/30 [17:00<1:25:29, 205.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 5 Loss: 0.2269\n",
      "Saved better model at epoch 5 with loss 0.2269\n",
      "\n",
      "Epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 6/30 [20:25<1:22:10, 205.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 6 Loss: 0.1518\n",
      "Saved better model at epoch 6 with loss 0.1518\n",
      "\n",
      "Epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 7/30 [23:51<1:18:45, 205.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 7 Loss: 0.1139\n",
      "Saved better model at epoch 7 with loss 0.1139\n",
      "\n",
      "Epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 8/30 [27:16<1:15:18, 205.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 8 Loss: 0.0906\n",
      "Saved better model at epoch 8 with loss 0.0906\n",
      "\n",
      "Epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 9/30 [30:41<1:11:48, 205.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 9 Loss: 0.0825\n",
      "Saved better model at epoch 9 with loss 0.0825\n",
      "\n",
      "Epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 10/30 [34:05<1:08:17, 204.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 10 Loss: 0.0723\n",
      "Saved better model at epoch 10 with loss 0.0723\n",
      "\n",
      "Epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 11/30 [37:30<1:04:52, 204.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 11 Loss: 0.0564\n",
      "Saved better model at epoch 11 with loss 0.0564\n",
      "\n",
      "Epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 12/30 [40:54<1:01:24, 204.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 12 Loss: 0.0499\n",
      "Saved better model at epoch 12 with loss 0.0499\n",
      "\n",
      "Epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 13/30 [44:18<57:55, 204.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 13 Loss: 0.0545\n",
      "\n",
      "Epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 14/30 [47:42<54:28, 204.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 14 Loss: 0.0439\n",
      "Saved better model at epoch 14 with loss 0.0439\n",
      "\n",
      "Epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 15/30 [51:06<51:02, 204.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 15 Loss: 0.0359\n",
      "Saved better model at epoch 15 with loss 0.0359\n",
      "\n",
      "Epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 16/30 [54:30<47:36, 204.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 16 Loss: 0.0362\n",
      "\n",
      "Epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 17/30 [57:53<44:11, 203.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 17 Loss: 0.0399\n",
      "\n",
      "Epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 18/30 [1:01:17<40:46, 203.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 18 Loss: 0.0359\n",
      "Saved better model at epoch 18 with loss 0.0359\n",
      "\n",
      "Epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 19/30 [1:04:41<37:21, 203.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 19 Loss: 0.0461\n",
      "\n",
      "Epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  67%|██████▋   | 20/30 [1:08:05<34:00, 204.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 20 Loss: 0.0333\n",
      "Saved better model at epoch 20 with loss 0.0333\n",
      "\n",
      "Epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 21/30 [1:11:29<30:36, 204.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 21 Loss: 0.0272\n",
      "Saved better model at epoch 21 with loss 0.0272\n",
      "\n",
      "Epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 22/30 [1:14:53<27:11, 203.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 22 Loss: 0.0289\n",
      "\n",
      "Epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 23/30 [1:18:17<23:47, 203.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 23 Loss: 0.0230\n",
      "Saved better model at epoch 23 with loss 0.0230\n",
      "\n",
      "Epoch 24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 24/30 [1:21:41<20:23, 203.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 24 Loss: 0.0226\n",
      "Saved better model at epoch 24 with loss 0.0226\n",
      "\n",
      "Epoch 25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 25/30 [1:25:04<16:58, 203.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 25 Loss: 0.0196\n",
      "Saved better model at epoch 25 with loss 0.0196\n",
      "\n",
      "Epoch 26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  87%|████████▋ | 26/30 [1:28:28<13:34, 203.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 26 Loss: 0.0172\n",
      "Saved better model at epoch 26 with loss 0.0172\n",
      "\n",
      "Epoch 27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 27/30 [1:31:51<10:11, 203.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 27 Loss: 0.0243\n",
      "\n",
      "Epoch 28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  93%|█████████▎| 28/30 [1:35:15<06:47, 203.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 28 Loss: 0.0510\n",
      "\n",
      "Epoch 29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  97%|█████████▋| 29/30 [1:38:39<03:23, 203.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 29 Loss: 0.0340\n",
      "\n",
      "Epoch 30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [1:42:02<00:00, 204.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 30 Loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def train_batch(model, images, text_encodes, text_lens, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    images = images.to(device)\n",
    "    text_encodes = text_encodes.to(device)\n",
    "    text_lens = text_lens.to(device)\n",
    "\n",
    "    logits = model(images)\n",
    "\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "    batch_size = logits.size(1)\n",
    "    input_lengths = torch.LongTensor([logits.size(0)] * batch_size)    \n",
    "    target_lengths = torch.flatten(text_lens)\n",
    "\n",
    "    loss = criterion(log_probs, text_encodes, input_lengths, target_lengths)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "IMAGE_PATH = '/kaggle/working/data/task2/image/images.json'\n",
    "TEXT_PATH = '/kaggle/working/data/task2/annotation/texts.json'\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "with open(IMAGE_PATH, 'r') as f:\n",
    "    image_paths = json.load(f)\n",
    "with open(TEXT_PATH, 'r') as f:\n",
    "    texts = json.load(f)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_paths, texts, test_size=0.3,\n",
    "                                                    shuffle=True, random_state=2020)\n",
    "\n",
    "train_dataset = ReceiptDataset(X_train, y_train)\n",
    "val_dataset = ReceiptDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "n_epochs = 30\n",
    "lr = 1e-5\n",
    "\n",
    "# model.load_state_dict(torch.load('./crnn.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "criterion = nn.CTCLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CRNN().to(device)\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "best_loss = float('inf') \n",
    "\n",
    "train_losses = []\n",
    "for epoch in tqdm(range(n_epochs), desc='Training'):\n",
    "    print(f'\\nEpoch {epoch + 1}: ')\n",
    "    train_batch_losses = []\n",
    "\n",
    "    for images, text_encodes, text_lens in train_dataloader:\n",
    "        loss = train_batch(model, images, text_encodes, text_lens, optimizer, criterion, device)\n",
    "        train_batch_losses.append(loss)\n",
    "\n",
    "    epoch_loss = sum(train_batch_losses) / len(train_batch_losses)\n",
    "    train_losses.append(epoch_loss)\n",
    "    print(f'===> Epoch {epoch + 1} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), 'crnn_model.pth')\n",
    "        print(f'Saved better model at epoch {epoch + 1} with loss {best_loss:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7503736,
     "sourceId": 11935298,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6181.53141,
   "end_time": "2025-05-25T09:40:00.659444",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-25T07:56:59.128034",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
